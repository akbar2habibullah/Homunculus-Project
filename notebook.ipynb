{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "\"\"\"\n",
    "This is the code to generate the custom tokenizer. Using Byte Pair Encoding\n",
    "\"\"\"\n",
    "\n",
    "def train_bpe_tokenizer(files, vocab_size=32000):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[IMG]\", \"[/IMG]\"]\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "    tokenizer.train(files, trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# Train and save the tokenizer\n",
    "tokenizer = train_bpe_tokenizer([\"train.txt\"])\n",
    "tokenizer.save(\"bpe_tokenizer_autoregressive.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "We use GeGLU (Gated GeLU) activation function\n",
    "\"\"\"\n",
    "\n",
    "class GeGLU(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(GeGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, embed_size)\n",
    "        self.fc2 = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.gelu(self.fc1(x)) * self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "This is the implementation of Root Mean Square normalization layer for replacing a standard normalization layer\n",
    "\"\"\"\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, embed_size, eps=1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        x = x / rms * self.scale\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the Rotary Positional Embedding parts. Consist of 1 dimensional for text sequence and 2 dimensional for image sequence.\n",
    "\"\"\"\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=500000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_dim=1):\n",
    "        # seq_len refers to the length of the sequence in the dimension where RoPE is applied\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, pos_emb):\n",
    "    sin, cos = pos_emb.chunk(2, dim=-1)\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "class RotaryPositionalEmbedding2D(nn.Module):\n",
    "    def __init__(self, dim, base=500000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, h, w):\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(h, device=self.inv_freq.device),\n",
    "                                        torch.arange(w, device=self.inv_freq.device))\n",
    "        grid = torch.stack((grid_y, grid_x), dim=-1).float()\n",
    "        sinusoid_inp = torch.einsum(\"...d,k->...dk\", grid, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        emb = emb.permute(2, 0, 1)\n",
    "        return emb\n",
    "\n",
    "def apply_rotary_pos_emb_2d(q, k, pos_emb):\n",
    "    sin, cos = pos_emb.chunk(2, dim=0)\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = x.reshape(x.shape[:-1] + (-1, 2))\n",
    "    x1, x2 = x.unbind(-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use Grouped Query Attention\n",
    "\"\"\"\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_groups):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        b, n, _ = q.shape\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.query(q).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(k).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(v).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Group queries\n",
    "        group_size = n // self.num_groups\n",
    "        q_groups = q.split(group_size, dim=2)\n",
    "        k_groups = k.split(group_size, dim=2)\n",
    "        v_groups = v.split(group_size, dim=2)\n",
    "\n",
    "        attn_output = torch.zeros_like(q)\n",
    "\n",
    "        for qg, kg, vg in zip(q_groups, k_groups, v_groups):\n",
    "            scores = torch.einsum('bhqd,bhkd->bhqk', qg, kg) / (self.head_dim ** 0.5)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            attn_output_group = torch.einsum('bhqk,bhkd->bhqd', attn_weights, vg)\n",
    "            attn_output += attn_output_group\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        attn_output = self.out(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is simple implementation of MLP for a certain layer that needs more than a single linear layer\n",
    "\"\"\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the code for the vision encoder part. Consist of similar block like the main Transformer, but we use 2D RoPE by default. The training objective is fill-in-the-middle objective and integrated seamlessly with the main text generation training pipeline.\n",
    "\"\"\"\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_groups):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        self.attention = GroupedQueryAttention(embed_size, num_heads, num_groups)\n",
    "        self.norm1 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.norm2 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.fc = nn.Sequential(\n",
    "            GeGLU(embed_size),\n",
    "        )\n",
    "        self.rotary_emb = RotaryPositionalEmbedding2D(self.head_dim)\n",
    "        \n",
    "    def forward(self, x, cache=None):\n",
    "        b, n, _ = x.shape\n",
    "        q = k = v = x\n",
    "        \n",
    "        # Split into heads and apply RoPE\n",
    "        q = q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        pos_emb = self.rotary_emb(q)\n",
    "        q, k = apply_rotary_pos_emb_2d(q, k, pos_emb)\n",
    "        \n",
    "        if cache is not None:\n",
    "            k = torch.cat([cache[0], k], dim=2)\n",
    "            v = torch.cat([cache[1], v], dim=2)\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        q = q.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        k = k.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        v = v.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        \n",
    "        attn_output, _ = self.attention(q, k, v)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        fc_output = self.fc(x)\n",
    "        x = self.norm2(x + fc_output)\n",
    "        \n",
    "        return x, (k, v)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_size, num_heads, num_layers, num_groups):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embedding = nn.Conv2d(3, embed_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            ViTBlock(embed_size, num_heads, num_groups) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "\n",
    "    def forward(self, x, use_cache=False, middle_training=False, mask_ratio=0.2, seed=None):\n",
    "        b, c, h, w = x.shape\n",
    "        x = self.patch_embedding(x)  # (B, embed_size, H/patch_size, W/patch_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_size)\n",
    "\n",
    "        # If enable fill-in-the-middle training\n",
    "        if middle_training:\n",
    "            # Deterministic masking if seed is pre-defined\n",
    "            if seed is not None:\n",
    "                torch.manual_seed(seed)\n",
    "            mask = torch.rand(b, self.num_patches) > mask_ratio\n",
    "            mask = mask.unsqueeze(-1).expand(x.size()).to(x.device)\n",
    "            masked_x = x * mask\n",
    "        else:\n",
    "            masked_x = x\n",
    "\n",
    "        # Initialize cache for storing key-value pairs\n",
    "        cache = [(None, None) for _ in range(len(self.layers))]\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if use_cache:\n",
    "                masked_x, cache[i] = layer(masked_x, cache=cache[i])\n",
    "            else:\n",
    "                masked_x, _ = layer(masked_x)\n",
    "\n",
    "        # If enable fill-in-the-middle training then return the MSE loss for the masked image patch\n",
    "        if middle_training:\n",
    "            loss = F.mse_loss(masked_x[mask == 0], x[mask == 0])\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        x = self.norm(masked_x)\n",
    "\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the main code containing the main Transformer backbone. Containing few mechanism:\n",
    "- Independent confidence layer for determine how many internal loop. Implemented as a few layers of MLP.\n",
    "- Blend the image embedding sequence into the text embedding sequence.\n",
    "- Selective Rotary Positional Encoding. Given image embedding sequence, the RoPE is applied 2 dimensionally.\n",
    "- Custom KV-caching based on the number of internal iterations. Making sure every internal iterations have independent KV-cache.\n",
    "\"\"\"\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_groups):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        self.attention = GroupedQueryAttention(embed_size, num_heads, num_groups)\n",
    "        self.norm1 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.norm2 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.fc = nn.Sequential(\n",
    "            GeGLU(embed_size),\n",
    "        )\n",
    "        self.rotary_emb = RotaryPositionalEmbedding(self.head_dim)\n",
    "        self.rotary_emb_2d = RotaryPositionalEmbedding2D(self.head_dim)\n",
    "        \n",
    "    def forward(self, x, cache=None, img_pos=[], end_img_pos=[]):\n",
    "        b, n, _ = x.shape\n",
    "        q = k = v = x.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply 1D RoPE by default\n",
    "        pos_emb = self.rotary_emb(q)\n",
    "        q, k = apply_rotary_pos_emb(q, k, pos_emb)\n",
    "        \n",
    "        # Apply 2D RoPE for image tokens\n",
    "        for start, end in zip(img_pos, end_img_pos):\n",
    "            pos_emb_2d = self.rotary_emb_2d(q[:, :, start:end])\n",
    "            q[:, :, start:end], k[:, :, start:end] = apply_rotary_pos_emb_2d(q[:, :, start:end], k[:, :, start:end], pos_emb_2d)\n",
    "        \n",
    "        if cache is not None:\n",
    "            k = torch.cat([cache[0], k], dim=2)\n",
    "            v = torch.cat([cache[1], v], dim=2)\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        q = q.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        k = k.transpose(1, 2).contiguous().view(b, -1, self.embed_size)  # -1 to account for cached tokens\n",
    "        v = v.transpose(1, 2).contiguous().view(b, -1, self.embed_size)  # -1 to account for cached tokens\n",
    "        \n",
    "        attn_output, _ = self.attention(q, k, v)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        fc_output = self.fc(x)\n",
    "        x = self.norm2(x + fc_output)\n",
    "        \n",
    "        return x, (k, v)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, context_size, img_size, patch_size, vit_layers, num_groups):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads, num_groups) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        self.confidence_fc = MLP(embed_size, embed_size // 2, 1, 3)  # Confidence prediction layer\n",
    "        self.context_size = context_size\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.vit = VisionTransformer(img_size, patch_size, embed_size, num_heads, vit_layers, num_groups)\n",
    "        self.img_token_id = self.embedding.num_embeddings - 2\n",
    "        self.end_img_token_id = self.embedding.num_embeddings - 1\n",
    "\n",
    "    def insert_image_embeddings(self, text_tensor, img_embeddings):\n",
    "        img_pos = (text_tensor == self.img_token_id).nonzero(as_tuple=True)\n",
    "        end_img_pos = (text_tensor == self.end_img_token_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        if len(img_pos[0]) != len(end_img_pos[0]) or len(img_pos[0]) != len(img_embeddings):\n",
    "            raise ValueError(\"Mismatch in number of image tokens and image embeddings\")\n",
    "        \n",
    "        new_tensor = text_tensor.clone()\n",
    "        offset = 0\n",
    "        for start, end, img_emb in zip(img_pos[0], end_img_pos[0], img_embeddings):\n",
    "            new_tensor = torch.cat((new_tensor[:start+1+offset], img_emb, new_tensor[end+offset:]), dim=1)\n",
    "            offset += img_emb.size(1) - (end - start - 1)\n",
    "        \n",
    "        return new_tensor, img_pos[0], end_img_pos[0]\n",
    "\n",
    "    def forward(self, x, imgs=None, num_iterations=1, use_cache=False, middle_training=False):\n",
    "        # middle_training: If True, use fill-in-the-middle objective for image training\n",
    "        # If False, use standard next-token prediction for text\n",
    "\n",
    "        img_seqs = []\n",
    "        vit_loss = 0\n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_embedding, loss = self.vit(img, use_cache=use_cache, middle_training=middle_training)\n",
    "                img_seqs.append(img_embedding)\n",
    "                vit_loss += loss\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        img_pos, end_img_pos = [], []\n",
    "        if img_seqs:\n",
    "            x, img_pos, end_img_pos = self.insert_image_embeddings(x, img_seqs)\n",
    "\n",
    "        caches = [[] for _ in range(len(self.layers))]\n",
    "        for _ in range(num_iterations):\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                if use_cache and caches[i]:\n",
    "                    x, caches[i] = layer(x, cache=caches[i][-1], img_pos=img_pos, end_img_pos=end_img_pos)\n",
    "                else:\n",
    "                    x, cache = layer(x, cache=None, img_pos=img_pos, end_img_pos=end_img_pos)\n",
    "        output = self.fc(x)\n",
    "        output = self.softmax(output)  # Apply softmax to the output logits\n",
    "        confidence = torch.sigmoid(self.confidence_fc(x.mean(dim=1)))  # Sigmoid for confidence score\n",
    "        if middle_training:\n",
    "            return output, confidence, vit_loss\n",
    "        else:\n",
    "            return output, confidence\n",
    "\n",
    "    def generate(self, input_text, tokenizer, max_length=128000, imgs=None, num_iterations=1, use_cache=False, beam_size=5):\n",
    "        tokens = tokenizer.encode(input_text).ids\n",
    "        input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
    "        \n",
    "        # Process images\n",
    "        img_seqs = []\n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_embedding, _ = self.vit(img, use_cache=use_cache)\n",
    "                img_seqs.append(img_embedding)\n",
    "        \n",
    "        if img_seqs:\n",
    "            input_tensor, img_pos, end_img_pos = self.insert_image_embeddings(input_tensor, img_seqs)\n",
    "        \n",
    "        # Initialize beam\n",
    "        beams = [(input_tensor, 0)]\n",
    "        \n",
    "        for _ in range(max_length - len(tokens)):\n",
    "            all_candidates = []\n",
    "            for beam, score in beams:\n",
    "                output, _ = self.forward(beam, num_iterations=num_iterations, use_cache=use_cache)\n",
    "                output = self.softmax(output)  # Apply softmax to the output logits\n",
    "                next_token_logits = output[0, -1, :]\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, beam_size)\n",
    "                \n",
    "                for logit, index in zip(top_k_logits, top_k_indices):\n",
    "                    new_beam = torch.cat((beam, index.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "                    new_score = score - logit.item()  # Negative log likelihood\n",
    "                    all_candidates.append((new_beam, new_score))\n",
    "            \n",
    "            # Select top beam_size candidates\n",
    "            beams = sorted(all_candidates, key=lambda x: x[1])[:beam_size]\n",
    "            \n",
    "            if beams[0][0][:, -1].item() == tokenizer.token_to_id(\"[SEP]\"):\n",
    "                break\n",
    "        \n",
    "        return tokenizer.decode(beams[0][0].squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "\"\"\"\n",
    "This is the code for saving and load the model with safetensors format\n",
    "\"\"\"\n",
    "\n",
    "def save_model_weights(model, path):\n",
    "    state_dict = model.state_dict()\n",
    "    save_file(state_dict, path)\n",
    "\n",
    "def load_model_weights(model, path):\n",
    "    state_dict = load_file(path)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.strategies import DeepSpeedStrategy\n",
    "from tokenizers import processors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define the constants\n",
    "VOCAB_SIZE = 128000\n",
    "EMBED_SIZE = 8192\n",
    "NUM_HEADS = 64\n",
    "NUM_LAYERS = 80\n",
    "CONTEXT_SIZE = 128000\n",
    "LEARNING_RATE = 1.5e-4\n",
    "NUM_EPOCHS = 10e5\n",
    "BASE_ITERATIONS = 1\n",
    "MAX_ITERATIONS = 10\n",
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "LOSS_THRESHOLD = 2.0  # Loss value threshold for increasing iterations\n",
    "IMG_SIZE = 1024\n",
    "PATCH_SIZE = 16\n",
    "VIT_LAYERS = 16\n",
    "NUM_GROUPS = 8  # Number of groups for Grouped Query Attention\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer_autoregressive.json\")\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# For pre-processing real dataset\n",
    "class DatasetLoader(Dataset):\n",
    "    def __init__(self, text_data, image_data):\n",
    "        self.text_data = text_data\n",
    "        self.image_data = image_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.tokenizer.encode(self.text_data[idx]).ids\n",
    "        image = self.image_data[idx]  # Assume this is already a tensor\n",
    "        return torch.tensor(text), image\n",
    "\n",
    "# dataset = DatasetLoader(text_data, image_data)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class TransformerLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, learning_rate):\n",
    "        super(TransformerLightningModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"[PAD]\"))\n",
    "        self.confidence_criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, imgs=None, num_iterations=1, use_cache=False, middle_training=False):\n",
    "        return self.model(x, imgs=imgs, num_iterations=num_iterations, use_cache=use_cache, middle_training=middle_training)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        example_input, imgs = batch\n",
    "        target = example_input.clone().detach()\n",
    "\n",
    "        # Shift target for autoregressive training while ignoring image token regions\n",
    "        target = target[:, 1:].contiguous().view(-1)\n",
    "        mask = (target != self.tokenizer.token_to_id(\"[IMG]\")) & (target != self.tokenizer.token_to_id(\"[/IMG]\"))\n",
    "        target = target[mask]\n",
    "\n",
    "        num_iterations = BASE_ITERATIONS\n",
    "        output, confidence, vit_loss = self(example_input[:, :-1], imgs=imgs, num_iterations=num_iterations, use_cache=True, middle_training=True)\n",
    "        output = output.view(-1, VOCAB_SIZE)[mask]\n",
    "        loss = self.criterion(output, target) + vit_loss\n",
    "        confidence_target = max(0, min(1, 1 - (loss.item() / LOSS_THRESHOLD)))\n",
    "        confidence_target = torch.tensor([[confidence_target]], dtype=torch.float, device=self.device)\n",
    "        confidence_loss = self.confidence_criterion(confidence, confidence_target)\n",
    "\n",
    "        while confidence.mean().item() < CONFIDENCE_THRESHOLD and num_iterations < MAX_ITERATIONS:\n",
    "            num_iterations += 1\n",
    "            output, confidence, vit_loss = self(example_input[:, :-1], imgs=imgs, num_iterations=num_iterations, use_cache=True, middle_training=True)\n",
    "            output = output.view(-1, VOCAB_SIZE)[mask]\n",
    "            loss = self.criterion(output, target) + vit_loss\n",
    "            confidence_target = max(0, min(1, 1 - (loss.item() / LOSS_THRESHOLD)))\n",
    "            confidence_target = torch.tensor([[confidence_target]], dtype=torch.float, device=self.device)\n",
    "            confidence_loss = self.confidence_criterion(confidence, confidence_target)\n",
    "\n",
    "        total_loss = loss + confidence_loss\n",
    "        self.log('train_loss', total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Create the model\n",
    "model = TransformerModel(VOCAB_SIZE, EMBED_SIZE, NUM_HEADS, NUM_LAYERS, CONTEXT_SIZE, IMG_SIZE, PATCH_SIZE, VIT_LAYERS, NUM_GROUPS)\n",
    "\n",
    "# Load model weights before training\n",
    "load_model_weights(model, \"model_weights.safetensors\")\n",
    "print(\"Model weights loaded.\")\n",
    "\n",
    "# Create the LightningModule\n",
    "lightning_model = TransformerLightningModule(model, tokenizer, LEARNING_RATE)\n",
    "\n",
    "# Define the DataLoader\n",
    "def train_dataloader():\n",
    "    # Example input (batch size 1, context size 512)\n",
    "    text = \"Your input text here with [IMG][/IMG] and [IMG][/IMG].\"\n",
    "    example_input = torch.tensor(tokenizer.encode(text).ids).unsqueeze(0)[:, :CONTEXT_SIZE]\n",
    "\n",
    "    # Example image inputs (batch size 1, 3 channels, 224x224)\n",
    "    imgs = [torch.randn(1, 3, 224, 224), torch.randn(1, 3, 224, 224)]\n",
    "\n",
    "    # return dataloader\n",
    "    return [(example_input, imgs)]\n",
    "\n",
    "# Define the Trainer with DeepSpeed and ZeRO-3\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    gpus=1,  # Use GPU if available\n",
    "    strategy=DeepSpeedStrategy(\n",
    "        stage=3,  # Use ZeRO-3\n",
    "        offload_optimizer=True,  # Offload optimizer states to CPU\n",
    "        offload_parameters=True,  # Offload model parameters to CPU\n",
    "        nvme_offload_dir=\"/path/to/nvme\",  # Path to NVMe storage for offloading\n",
    "    ),\n",
    "    callbacks=[ModelCheckpoint(monitor='train_loss')]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(lightning_model, train_dataloaders=train_dataloader())\n",
    "\n",
    "# Save model weights at the end of training\n",
    "save_model_weights(model, \"model_weights.safetensors\")\n",
    "print(\"Model weights saved.\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
