{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch tokenizers safetensors pytorch-lightning deepspeed peft torchvision bitsandbytes bitnet flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "We use GeGLU (Gated GeLU) activation function\n",
    "\"\"\"\n",
    "\n",
    "class GeGLU(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(GeGLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, embed_size)\n",
    "        self.fc2 = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.gelu(self.fc1(x)) * self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "This is the implementation of Root Mean Square normalization layer for replacing a standard normalization layer\n",
    "\"\"\"\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, embed_size, eps=1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        x = x / rms * self.scale\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the Rotary Positional Embedding parts. Consist of 1 dimensional for text sequence and 2 dimensional for image sequence.\n",
    "\"\"\"\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=500000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_dim=1):\n",
    "        # seq_len refers to the length of the sequence in the dimension where RoPE is applied\n",
    "        seq_len = x.shape[seq_dim]\n",
    "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, pos_emb):\n",
    "    sin, cos = pos_emb.chunk(2, dim=-1)\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "class RotaryPositionalEmbedding2D(nn.Module):\n",
    "    def __init__(self, dim, base=500000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, h, w):\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(h, device=self.inv_freq.device),\n",
    "                                        torch.arange(w, device=self.inv_freq.device))\n",
    "        grid = torch.stack((grid_y, grid_x), dim=-1).float()\n",
    "        sinusoid_inp = torch.einsum(\"...d,k->...dk\", grid, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        emb = emb.permute(2, 0, 1)\n",
    "        return emb\n",
    "\n",
    "def apply_rotary_pos_emb_2d(q, k, pos_emb):\n",
    "    sin, cos = pos_emb.chunk(2, dim=0)\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = x.reshape(x.shape[:-1] + (-1, 2))\n",
    "    x1, x2 = x.unbind(-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use Grouped Query Attention\n",
    "\"\"\"\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_groups):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        b, n, _ = q.shape\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.query(q).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(k).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(v).view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Group queries\n",
    "        group_size = n // self.num_groups\n",
    "        q_groups = q.split(group_size, dim=2)\n",
    "        k_groups = k.split(group_size, dim=2)\n",
    "        v_groups = v.split(group_size, dim=2)\n",
    "\n",
    "        attn_output = torch.zeros_like(q)\n",
    "\n",
    "        for qg, kg, vg in zip(q_groups, k_groups, v_groups):\n",
    "            scores = torch.einsum('bhqd,bhkd->bhqk', qg, kg) / (self.head_dim ** 0.5)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            attn_output_group = torch.einsum('bhqk,bhkd->bhqd', attn_weights, vg)\n",
    "            attn_output += attn_output_group\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        attn_output = self.out(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is simple implementation of MLP for a certain layer that needs more than a single linear layer\n",
    "\"\"\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn import flash_attn_func\n",
    "\n",
    "\"\"\"\n",
    "This is option for using both Grouped Query Attention and Flash Attention\n",
    "\"\"\"\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_groups):\n",
    "        super(FlashAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_size, embed_size)\n",
    "        self.k_proj = nn.Linear(embed_size, embed_size // (num_heads // num_groups))\n",
    "        self.v_proj = nn.Linear(embed_size, embed_size // (num_heads // num_groups))\n",
    "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        b, n, _ = q.shape\n",
    "        \n",
    "        q = self.q_proj(q).view(b, n, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(k).view(b, n, self.num_groups, self.head_dim)\n",
    "        v = self.v_proj(v).view(b, n, self.num_groups, self.head_dim)\n",
    "        \n",
    "        # Repeat k and v to match the number of heads\n",
    "        k = k.repeat_interleave(self.num_heads // self.num_groups, dim=2)\n",
    "        v = v.repeat_interleave(self.num_heads // self.num_groups, dim=2)\n",
    "        \n",
    "        # Prepare inputs for flash_attn_func\n",
    "        q = q.transpose(1, 2)  # [b, nh, n, hd]\n",
    "        k = k.transpose(1, 2)  # [b, nh, n, hd]\n",
    "        v = v.transpose(1, 2)  # [b, nh, n, hd]\n",
    "\n",
    "        attn_output = flash_attn_func(q, k, v, softmax_scale=None)\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        out = self.out_proj(attn_output)\n",
    "        \n",
    "        return out, None  # Return None for compatibility with existing implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the code for the vision encoder part. Consist of similar block like the main Transformer, but we use 2D RoPE by default. The training objective is fill-in-the-middle objective and integrated seamlessly with the main text generation training pipeline.\n",
    "\"\"\"\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_groups, use_flash_attention=False):\n",
    "        super(ViTBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        if use_flash_attention:\n",
    "            self.attention = FlashAttention(embed_size, num_heads, num_groups)\n",
    "        else:\n",
    "            self.attention = GroupedQueryAttention(embed_size, num_heads, num_groups)\n",
    "        self.norm1 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.norm2 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.fc = nn.Sequential(\n",
    "            GeGLU(embed_size),\n",
    "        )\n",
    "        self.rotary_emb = RotaryPositionalEmbedding2D(self.head_dim)\n",
    "        \n",
    "    def forward(self, x, cache=None):\n",
    "        b, n, _ = x.shape\n",
    "        q = k = v = x\n",
    "        \n",
    "        # Split into heads and apply RoPE\n",
    "        q = q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        pos_emb = self.rotary_emb(q)\n",
    "        q, k = apply_rotary_pos_emb_2d(q, k, pos_emb)\n",
    "        \n",
    "        if cache is not None:\n",
    "            k = torch.cat([cache[0], k], dim=2)\n",
    "            v = torch.cat([cache[1], v], dim=2)\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        q = q.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        k = k.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        v = v.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        \n",
    "        attn_output, _ = self.attention(q, k, v)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        fc_output = self.fc(x)\n",
    "        x = self.norm2(x + fc_output)\n",
    "        \n",
    "        return x, (k, v)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_size, num_heads, num_layers, num_groups, use_flash_attention=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embedding = nn.Conv2d(3, embed_size, kernel_size=patch_size, stride=patch_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            ViTBlock(embed_size, num_heads, num_groups, use_flash_attention) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "\n",
    "    def forward(self, x, use_cache=False, middle_training=False, mask_ratio=0.2, seed=None):\n",
    "        b, c, h, w = x.shape\n",
    "        x = self.patch_embedding(x)  # (B, embed_size, H/patch_size, W/patch_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_size)\n",
    "\n",
    "        # If enable fill-in-the-middle training\n",
    "        if middle_training:\n",
    "            # Deterministic masking if seed is pre-defined\n",
    "            if seed is not None:\n",
    "                torch.manual_seed(seed)\n",
    "            mask = torch.rand(b, self.num_patches) > mask_ratio\n",
    "            mask = mask.unsqueeze(-1).expand(x.size()).to(x.device)\n",
    "            masked_x = x * mask\n",
    "        else:\n",
    "            masked_x = x\n",
    "\n",
    "        # Initialize cache for storing key-value pairs\n",
    "        cache = [(None, None) for _ in range(len(self.layers))]\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if use_cache:\n",
    "                masked_x, cache[i] = layer(masked_x, cache=cache[i])\n",
    "            else:\n",
    "                masked_x, _ = layer(masked_x)\n",
    "\n",
    "        # If enable fill-in-the-middle training then return the MSE loss for the masked image patch\n",
    "        if middle_training:\n",
    "            loss = F.mse_loss(masked_x[mask == 0], x[mask == 0])\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        x = self.norm(masked_x)\n",
    "\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the main code containing the main Transformer backbone. Containing few mechanism:\n",
    "- Independent confidence layer for determine how many internal loop. Implemented as a few layers of MLP.\n",
    "- Blend the image embedding sequence into the text embedding sequence.\n",
    "- Selective Rotary Positional Encoding. Given image embedding sequence, the RoPE is applied 2 dimensionally.\n",
    "- Custom KV-caching based on the number of internal iterations. Making sure every internal iterations have independent KV-cache.\n",
    "- Flash Attention option.\n",
    "\"\"\"\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_groups, use_flash_attention=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        if use_flash_attention:\n",
    "            self.attention = FlashAttention(embed_size, num_heads, num_groups)\n",
    "        else:\n",
    "            self.attention = GroupedQueryAttention(embed_size, num_heads, num_groups)\n",
    "        self.norm1 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.norm2 = RMSNorm(embed_size)  # Use RMSNorm instead of LayerNorm\n",
    "        self.fc = nn.Sequential(\n",
    "            GeGLU(embed_size),\n",
    "        )\n",
    "        self.rotary_emb = RotaryPositionalEmbedding(self.head_dim)\n",
    "        self.rotary_emb_2d = RotaryPositionalEmbedding2D(self.head_dim)\n",
    "        \n",
    "    def forward(self, x, cache=None, img_pos=[], end_img_pos=[]):\n",
    "        b, n, _ = x.shape\n",
    "        q = k = v = x.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply 1D RoPE by default\n",
    "        pos_emb = self.rotary_emb(q)\n",
    "        q, k = apply_rotary_pos_emb(q, k, pos_emb)\n",
    "        \n",
    "        # Apply 2D RoPE for image tokens\n",
    "        for start, end in zip(img_pos, end_img_pos):\n",
    "            pos_emb_2d = self.rotary_emb_2d(q[:, :, start:end])\n",
    "            q[:, :, start:end], k[:, :, start:end] = apply_rotary_pos_emb_2d(q[:, :, start:end], k[:, :, start:end], pos_emb_2d)\n",
    "        \n",
    "        if cache is not None:\n",
    "            k = torch.cat([cache[0], k], dim=2)\n",
    "            v = torch.cat([cache[1], v], dim=2)\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        q = q.transpose(1, 2).contiguous().view(b, n, self.embed_size)\n",
    "        k = k.transpose(1, 2).contiguous().view(b, -1, self.embed_size)  # -1 to account for cached tokens\n",
    "        v = v.transpose(1, 2).contiguous().view(b, -1, self.embed_size)  # -1 to account for cached tokens\n",
    "        \n",
    "        attn_output, _ = self.attention(q, k, v)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        fc_output = self.fc(x)\n",
    "        x = self.norm2(x + fc_output)\n",
    "        \n",
    "        return x, (k, v)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, context_size, img_size, patch_size, vit_layers, num_groups, use_flash_attention=False):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads, num_groups, use_flash_attention) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        self.confidence_fc = MLP(embed_size, embed_size // 2, 1, 3)  # Confidence prediction layer\n",
    "        self.context_size = context_size\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.vit = VisionTransformer(img_size, patch_size, embed_size, num_heads, vit_layers, num_groups, use_flash_attention)\n",
    "        self.img_token_id = self.embedding.num_embeddings - 2\n",
    "        self.end_img_token_id = self.embedding.num_embeddings - 1\n",
    "\n",
    "    def insert_image_embeddings(self, text_tensor, img_embeddings):\n",
    "        img_pos = (text_tensor == self.img_token_id).nonzero(as_tuple=True)\n",
    "        end_img_pos = (text_tensor == self.end_img_token_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        if len(img_pos[0]) != len(end_img_pos[0]) or len(img_pos[0]) != len(img_embeddings):\n",
    "            raise ValueError(\"Mismatch in number of image tokens and image embeddings\")\n",
    "        \n",
    "        new_tensor = text_tensor.clone()\n",
    "        offset = 0\n",
    "        for start, end, img_emb in zip(img_pos[0], end_img_pos[0], img_embeddings):\n",
    "            new_tensor = torch.cat((new_tensor[:start+1+offset], img_emb, new_tensor[end+offset:]), dim=1)\n",
    "            offset += img_emb.size(1) - (end - start - 1)\n",
    "        \n",
    "        return new_tensor, img_pos[0], end_img_pos[0]\n",
    "\n",
    "    def forward(self, x, imgs=None, num_iterations=1, use_cache=False, middle_training=False):\n",
    "        # middle_training: If True, use fill-in-the-middle objective for image training\n",
    "        # If False, use standard next-token prediction for text\n",
    "\n",
    "        img_seqs = []\n",
    "        vit_loss = 0\n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_embedding, loss = self.vit(img, use_cache=use_cache, middle_training=middle_training)\n",
    "                img_seqs.append(img_embedding)\n",
    "                vit_loss += loss\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        img_pos, end_img_pos = [], []\n",
    "        if img_seqs:\n",
    "            x, img_pos, end_img_pos = self.insert_image_embeddings(x, img_seqs)\n",
    "\n",
    "        caches = [[] for _ in range(len(self.layers))]\n",
    "        for _ in range(num_iterations):\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                if use_cache and caches[i]:\n",
    "                    x, caches[i] = layer(x, cache=caches[i][-1], img_pos=img_pos, end_img_pos=end_img_pos)\n",
    "                else:\n",
    "                    x, cache = layer(x, cache=None, img_pos=img_pos, end_img_pos=end_img_pos)\n",
    "        output = self.fc(x)\n",
    "        output = self.softmax(output)  # Apply softmax to the output logits\n",
    "        confidence = torch.sigmoid(self.confidence_fc(x.mean(dim=1)))  # Sigmoid for confidence score\n",
    "        if middle_training:\n",
    "            return output, confidence, vit_loss\n",
    "        else:\n",
    "            return output, confidence\n",
    "\n",
    "    def generate(self, input_text, tokenizer, max_length=128000, imgs=None, num_iterations=1, use_cache=False, beam_size=5):\n",
    "        tokens = tokenizer.encode(input_text).ids\n",
    "        input_tensor = torch.tensor(tokens).unsqueeze(0)\n",
    "        \n",
    "        # Process images\n",
    "        img_seqs = []\n",
    "        if imgs is not None:\n",
    "            for img in imgs:\n",
    "                img_embedding, _ = self.vit(img, use_cache=use_cache)\n",
    "                img_seqs.append(img_embedding)\n",
    "        \n",
    "        if img_seqs:\n",
    "            input_tensor, img_pos, end_img_pos = self.insert_image_embeddings(input_tensor, img_seqs)\n",
    "        \n",
    "        # Initialize beam\n",
    "        beams = [(input_tensor, 0)]\n",
    "        \n",
    "        for _ in range(max_length - len(tokens)):\n",
    "            all_candidates = []\n",
    "            for beam, score in beams:\n",
    "                output, _ = self.forward(beam, num_iterations=num_iterations, use_cache=use_cache)\n",
    "                output = self.softmax(output)  # Apply softmax to the output logits\n",
    "                next_token_logits = output[0, -1, :]\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, beam_size)\n",
    "                \n",
    "                for logit, index in zip(top_k_logits, top_k_indices):\n",
    "                    new_beam = torch.cat((beam, index.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "                    new_score = score - logit.item()  # Negative log likelihood\n",
    "                    all_candidates.append((new_beam, new_score))\n",
    "            \n",
    "            # Select top beam_size candidates\n",
    "            beams = sorted(all_candidates, key=lambda x: x[1])[:beam_size]\n",
    "            \n",
    "            if beams[0][0][:, -1].item() == tokenizer.token_to_id(\"[SEP]\"):\n",
    "                break\n",
    "        \n",
    "        return tokenizer.decode(beams[0][0].squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file, load_file\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "This is the code for saving and loading the model with safetensors format,\n",
    "dividing the model weights into N amount files and generating an index file.\n",
    "\"\"\"\n",
    "\n",
    "def save_model_weights(model, base_path, num_files=1):\n",
    "    state_dict = model.state_dict()\n",
    "    keys = list(state_dict.keys())\n",
    "    chunk_size = len(keys) // num_files\n",
    "\n",
    "    index = {}\n",
    "\n",
    "    for i in range(num_files):\n",
    "        chunk_keys = keys[i * chunk_size:(i + 1) * chunk_size]\n",
    "        chunk_state_dict = {key: state_dict[key] for key in chunk_keys}\n",
    "        save_file(chunk_state_dict, f\"{base_path}_part_{i}.safetensors\")\n",
    "        index[f\"{base_path}_part_{i}.safetensors\"] = list(chunk_state_dict.keys())\n",
    "\n",
    "    with open(f\"{base_path}.index.json\", 'w') as f:\n",
    "        json.dump(index, f, indent=4)\n",
    "\n",
    "def load_model_weights(model, base_path, num_files=1):\n",
    "    state_dict = {}\n",
    "\n",
    "    with open(f\"{base_path}.index.json\", 'r') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    for i in range(num_files):\n",
    "        file_path = f\"{base_path}_part_{i}.safetensors\"\n",
    "        if file_path in index:\n",
    "            chunk_state_dict = load_file(file_path)\n",
    "            state_dict.update(chunk_state_dict)\n",
    "\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from tokenizer import Tokenizer\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DeepSpeedStrategy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the constants\n",
    "VOCAB_SIZE = 128000\n",
    "EMBED_SIZE = 8192\n",
    "NUM_HEADS = 64\n",
    "NUM_LAYERS = 80\n",
    "CONTEXT_SIZE = 128000\n",
    "LEARNING_RATE = 1.5e-4\n",
    "NUM_EPOCHS = 10\n",
    "BASE_ITERATIONS = 1\n",
    "MAX_ITERATIONS = 10\n",
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "LOSS_THRESHOLD = 2.0  # Loss value threshold for increasing iterations\n",
    "IMG_SIZE = 1024\n",
    "PATCH_SIZE = 16\n",
    "VIT_LAYERS = 16\n",
    "NUM_GROUPS = 8  # Number of groups for Grouped Query Attention\n",
    "BATCH_SIZE = 4\n",
    "USE_FLASH_ATTENTION = False  # Set this to True to use Flash Attention\n",
    "\n",
    "\"\"\"\n",
    "This is the scripts for LoRA finetuning.\n",
    "\"\"\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer_autoregressive.json\")\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define a dataset with both text and images\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, transform):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, image_path = self.data[idx]\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        input_ids = torch.tensor(encoded.ids)\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return input_ids, image\n",
    "\n",
    "# PyTorch Lightning Module\n",
    "class TransformerLightningModule(pl.LightningModule):\n",
    "    def __init__(self, base_model, lora_config):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.model = get_peft_model(self.base_model, lora_config)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.confidence_criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, imgs, num_iterations=1):\n",
    "        return self.model(input_ids, imgs=imgs, num_iterations=num_iterations, use_cache=True, middle_training=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, images = batch\n",
    "        target = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        num_iterations = BASE_ITERATIONS\n",
    "        outputs, confidence, vit_loss = self(input_ids[:, :-1], imgs=images, num_iterations=num_iterations)\n",
    "        \n",
    "        loss = self.criterion(outputs.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "        total_loss = loss + vit_loss\n",
    "        \n",
    "        confidence_target = torch.clamp(1 - (total_loss.detach() / LOSS_THRESHOLD), 0, 1)\n",
    "        confidence_loss = self.confidence_criterion(confidence, confidence_target)\n",
    "        \n",
    "        total_loss += confidence_loss\n",
    "\n",
    "        while confidence.mean().item() < CONFIDENCE_THRESHOLD and num_iterations < MAX_ITERATIONS:\n",
    "            num_iterations += 1\n",
    "            outputs, confidence, vit_loss = self(input_ids[:, :-1], imgs=images, num_iterations=num_iterations)\n",
    "            \n",
    "            loss = self.criterion(outputs.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "            iter_total_loss = loss + vit_loss\n",
    "            \n",
    "            confidence_target = torch.clamp(1 - (iter_total_loss.detach() / LOSS_THRESHOLD), 0, 1)\n",
    "            confidence_loss = self.confidence_criterion(confidence, confidence_target)\n",
    "            \n",
    "            iter_total_loss += confidence_loss\n",
    "            total_loss += iter_total_loss\n",
    "\n",
    "        self.log('train_loss', total_loss)\n",
    "        self.log('confidence', confidence.mean())\n",
    "        self.log('num_iterations', num_iterations)\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Main training function\n",
    "def train_model():\n",
    "    # Create the base model\n",
    "    base_model = TransformerModel(VOCAB_SIZE, EMBED_SIZE, NUM_HEADS, NUM_LAYERS, CONTEXT_SIZE, IMG_SIZE, PATCH_SIZE, VIT_LAYERS, NUM_GROUPS, USE_FLASH_ATTENTION)\n",
    "\n",
    "    # Load pre-trained weights\n",
    "    load_model_weights(base_model, \"model_weights\", num_files=4)\n",
    "\n",
    "    # Identify and name the layers you want to adapt\n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        layer.attention.query.name = f'main_transformer.layers.{i}.attention.query'\n",
    "        layer.attention.key.name = f'main_transformer.layers.{i}.attention.key'\n",
    "        layer.attention.value.name = f'main_transformer.layers.{i}.attention.value'\n",
    "        layer.attention.out.name = f'main_transformer.layers.{i}.attention.out'\n",
    "\n",
    "    for i, layer in enumerate(base_model.vit.layers):\n",
    "        layer.attention.query.name = f'vit.layers.{i}.attention.query'\n",
    "        layer.attention.key.name = f'vit.layers.{i}.attention.key'\n",
    "        layer.attention.value.name = f'vit.layers.{i}.attention.value'\n",
    "        layer.attention.out.name = f'vit.layers.{i}.attention.out'\n",
    "\n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"main_transformer.layers.*.attention.query\",\n",
    "            \"main_transformer.layers.*.attention.key\",\n",
    "            \"main_transformer.layers.*.attention.value\",\n",
    "            \"main_transformer.layers.*.attention.out\",\n",
    "            \"vit.layers.*.attention.query\",\n",
    "            \"vit.layers.*.attention.key\",\n",
    "            \"vit.layers.*.attention.value\",\n",
    "            \"vit.layers.*.attention.out\",\n",
    "            \"confidence_fc.*\",  # Include confidence layer in LoRA\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "\n",
    "    # Create Lightning module\n",
    "    model = TransformerLightningModule(base_model, lora_config)\n",
    "\n",
    "    # Sample data (replace with your dataset)\n",
    "    data = [\n",
    "        (\"This is a sample text with an image [IMG][/IMG]\", \"path/to/image1.jpg\"),\n",
    "        (\"Another example of text and image [IMG][/IMG] data.\", \"path/to/image2.jpg\"),\n",
    "        # Add more text-image pairs...\n",
    "    ]\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TextImageDataset(data, tokenizer, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Define DeepSpeed config\n",
    "    deepspeed_config = {\n",
    "        \"train_batch_size\": BATCH_SIZE,\n",
    "        \"fp16\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"sub_group_size\": 1e9,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"stage3_max_live_parameters\": 1e9,\n",
    "            \"stage3_max_reuse_distance\": 1e9,\n",
    "            \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath='checkpoints',\n",
    "        filename='model-{epoch:02d}-{train_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        monitor='train_loss'\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        strategy=DeepSpeedStrategy(config=deepspeed_config),\n",
    "        precision=16,  # Use mixed precision\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1  # Adjust based on your GPU setup\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    # Save the fine-tuned LoRA weights\n",
    "    model.model.save_pretrained(\"lora_weights\")\n",
    "\n",
    "    print(\"Fine-tuning completed. LoRA weights saved.\")\n",
    "\n",
    "    # Merge the LoRA weights with the base model\n",
    "    merged_model = model.model.merge_and_unload()\n",
    "\n",
    "    # Save the merged model\n",
    "    save_model_weights(merged_model, \"merged_model_weights\", num_files=4)\n",
    "\n",
    "    print(\"LoRA weights merged with base model and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
